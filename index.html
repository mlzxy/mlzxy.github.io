<!DOCTYPE html>

<head>
    <title>Xinyu Zhang - Homepage</title>
    <meta name="author" content="Xinyu Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="preconnect" href="https://fonts.gstatic.com/">
    <link rel="stylesheet" href="css/stylesheet.css">
    <link rel="icon" type="image/png" href="images/robot.png">
</head>

<script>
    let fPressCount = 0;
    let timer;

    document.addEventListener('keydown', function (event) {
        if (event.key === 'f' || event.key === 'F') {
            fPressCount++;
            if (fPressCount === 5) {
                triggerFunction();
                resetCounter();
            }
            clearTimeout(timer);
            timer = setTimeout(resetCounter, 1000); // 1-second delay to reset
        }
    });

    function resetCounter() {
        fPressCount = 0;
    }

    function triggerFunction() {
        const elements = document.querySelectorAll('.ppt');
        elements.forEach(element => {
            const computedStyle = window.getComputedStyle(element);
            if (computedStyle.visibility === 'hidden' || computedStyle.height === '0px') {
                element.style.visibility = 'visible';
                element.style.height = 'auto';
            } else {
                element.style.visibility = 'hidden';
                element.style.height = '0';
            }
        });
    }

</script>

<body data-new-gr-c-s-check-loaded="14.1199.0" data-gr-ext-installed="">
    <table id="main"
        style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:5%;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <hr>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:25%;max-width:25%">
                                    <img style="width:100%;max-width:100%;box-shadow: 0 1px 3px rgba(0,0,0,0.12), 0 1px 2px rgba(0,0,0,0.24);"
                                        alt="profile photo" src="images/me.jpg">
                                </td>
                                <td style="padding:2.5%;width:75%;vertical-align:middle">
                                    <p style="text-align:left">
                                        <name>Xinyu Zhang (张鑫语)</name>
                                    </p>
                                    <p>
                                        I am a third-year PhD student in Computer Science at Rutgers University, advised
                                        by <a href="http://rl.cs.rutgers.edu/">Prof. Abdeslam Boularias</a>.
                                    </p>
                                    <p>
                                        Before joining Rutgers, I worked at Microsoft and Megvii (Face++) as a software
                                        engineer.
                                    </p>
                                    <p>
                                        Previously, I earned a Master's at University of California San Diego (UCSD),
                                        advised by <a href="http://dsp.ucsd.edu/~kreutz/">Prof. Ken Kreutz-Delgado</a>.
                                        I earned a Bachelor at University of Science and Technology of China (USTC).
                                    </p>
                                    <p style="text-align:left">
                                        <a href="https://scholar.google.com/citations?user=M7hnG9oAAAAJ&hl=en">Google
                                            Scholar</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://github.com/mlzxy">GitHub</a>
                                        <br> Email: xz653 at rutgers dot edu
                                    </p>
                                </td>

                            </tr>
                        </tbody>
                    </table>
                    <hr>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding-left:20px;padding-bottom:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p style="text-align:justify">
                                        I am a third-year PhD student at Rutgers University. During my PhD, I have
                                        published five first-author papers in top-tier robotics and AI conferences,
                                        including RSS, RA-L, CoRL, and IROS. These works have received hundreds of stars
                                        at GitHub. I have a strong foundation in 2D/3D computer vision, representation
                                        learning, vision-language models, and policy learning for intelligent agents.
                                        <!-- Specifically, I am interested in two robot learning problems: 1) how to learn a universal low-level policy for various manipulation tasks, and 2) how to learn a generalizable high-level planning ability. My previous work focus on the first problem, for example, <a href="https://github.com/mlzxy/arp" target="_blank">autoregressive policy</a>. I am currently working on the second problem, which I believe the key lies in the inter-disciplinary understanding of language, vision and decision making.  -->
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <table
                        style="width:100%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding-left:20px;padding-bottom:20px;width:33%;vertical-align:top">
                                    <video width="280" class="shadow-border" height="" playsinline="" autoplay=""
                                        muted="" loop="" style="border-radius: 5px;">
                                        <source src="publications/arxiv/mbgs.mp4" type="video/mp4">
                                    </video>
                                </td>
                                <td width="75%" valign="top">
                                    <papertitle>Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction
                                    </papertitle>
                                    <p>
                                        <author>Xinyu Zhang</author>, Haonan Chang, Yuhan Liu, Abdeslam Boularias
                                        <br>
                                        <i>Preprint
                                        </i>
                                        <br>
                                        <a href="https://arxiv.org/abs/2503.09040" target="_blank">PDF</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://mlzxy.github.io/motion-blender-gs/" target="_blank">Website</a>
                                    </p>

                                    <div class="ppt">
                                        <ul>
                                            <li>todo</li>
                                        </ul>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>



                    <table
                        style="width:100%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding-left:20px;padding-bottom:20px;width:33%;vertical-align:top">
                                    <video width="280" class="shadow-border" height="" playsinline="" autoplay=""
                                        muted="" loop="" style="border-radius: 5px;">
                                        <source src="publications/iclr2025/demo.mp4" type="video/mp4">
                                    </video>
                                </td>
                                <td width="75%" valign="top">
                                    <papertitle>Autoregressive Action Sequence Learning for Robotic Manipulation
                                    </papertitle>
                                    <p>
                                        <author>Xinyu Zhang</author>, Yuhan Liu, Haonan Chang, Liam Schramm, Abdeslam
                                        Boularias
                                        <br>
                                        <i>IEEE Robotics and Automation Letters (RA-L) 2025
                                        </i>
                                        <br>
                                        <a href="https://arxiv.org/abs/2410.03132" target="_blank">PDF</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://github.com/mlzxy/arp" target="_blank">Code & Video</a>
                                    </p>

                                    <div class="ppt">
                                        <ul>
                                            <li>Robot actions as a language, but robot actions are heterogeneous and
                                                often continuous.</li>
                                            <li>We propose chunking causal transformer to adapt autoregressive models
                                                for robot actions</li>
                                            <li>A universal architecture that establishes new state of the art in
                                                Push-T, ALOHA, and RLBench.</li>
                                        </ul>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding-left:20px;padding-bottom:20px;width:33%;vertical-align:top">
                                    <video width="280" class="shadow-border" height="" playsinline="" autoplay=""
                                        muted="" loop="" style="border-radius: 5px;">
                                        <source src="publications/corl2024/devit.mp4" type="video/mp4">
                                    </video>
                                </td>
                                <td width="75%" valign="top">
                                    <papertitle>Detect Everything with Few Examples
                                    </papertitle>
                                    <p>
                                        <author>Xinyu Zhang</author>, Yuhan Liu, Yuting Wang, Abdeslam Boularias
                                        <br>
                                        <i>Conference on Robot Learning (CoRL) 2024
                                        </i>
                                        <br>
                                        <a href="https://arxiv.org/abs/2309.12969" target="_blank">PDF</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://github.com/mlzxy/devit" target="_blank">Code</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://mlzxy.github.io/devit" target="_blank">Website</a>
                                    </p>
                                    <div class="ppt">
                                        <ul>
                                            <li>Existing work mix representation learning with detection</li>
                                            <li>We don't learn representation, but focus on how to use existing
                                                pretrained ones</li>
                                            <li>Detect by propagating ROI regions in attention map</li>
                                        </ul>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <table
                        style="width:100%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding-left:20px;padding-bottom:20px;width:33%;vertical-align:top">
                                    <video width="280" class="shadow-border" height="" playsinline="" autoplay=""
                                        muted="" loop="" style="border-radius: 5px;">
                                        <source src="publications/corl2024/vkt.mp4" type="video/mp4">
                                    </video>
                                </td>
                                <td width="75%" valign="top">
                                    <papertitle>Scaling Manipulation Learning with Visual Kinematic Chain Prediction
                                    </papertitle>
                                    <p>
                                        <author>Xinyu Zhang</author>, Yuhan Liu, Haonan Chang, Abdeslam Boularias
                                        <br>
                                        <i>Conference on Robot Learning (CoRL) 2024
                                        </i>
                                        <br>
                                        <a href="https://arxiv.org/abs/2406.07837" target="_blank">PDF</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://github.com/mlzxy/visual-kinetic-chain" target="_blank">Code</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://mlzxy.github.io/visual-kinetic-chain/"
                                            target="_blank">Website</a>
                                    </p>
                                    <div class="ppt">
                                        <ul>
                                            <li>How to learn a single policy for diverse environments? </li>
                                            <li>Use a universal, visually grounded, analytically determined action
                                                space!</li>
                                            <li>That is, the visual projection of the robot kinematic structure</li>
                                        </ul>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding-left:20px;padding-bottom:20px;width:33%;vertical-align:top">
                                    <video width="280" class="shadow-border" height="" playsinline="" autoplay=""
                                        muted="" loop="" style="border-radius: 5px;">
                                        <source src="publications/rss2024/demo.mp4" type="video/mp4">
                                    </video>
                                </td>
                                <td width="75%" valign="top">
                                    <papertitle>One-Shot Imitation Learning with Invariance Matching for Robotic
                                        Manipulation
                                    </papertitle>
                                    <p>
                                        <author>Xinyu Zhang</author>, Abdeslam Boularias
                                        <br>
                                        <i>Robotics: Science and Systems (RSS) 2024
                                        </i>
                                        <br>
                                        <a href="https://arxiv.org/abs/2405.13178" target="_blank">PDF</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://github.com/mlzxy/imop" target="_blank">Code</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://mlzxy.github.io/imop" target="_blank">Website</a>
                                    </p>

                                    <div class="ppt">
                                        <ul>
                                            <li>Bind 3D spatial regions to robot actions, so actions have semantics!
                                            </li>
                                            <li>Learn to discover these "key regions" and match regions between
                                                demonstrations </li>
                                            <li>Through region matching, we transfer robot actions to new scenes in
                                                one-shot.</li>
                                        </ul>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <table
                        style="width:100%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding-left:20px;padding-bottom:20px;width:33%;vertical-align:top">
                                    <video width="280" class="shadow-border" height="" playsinline="" autoplay=""
                                        muted="" loop="" style="border-radius: 5px;">
                                        <source src="publications/iros2024/DAP.mp4" type="video/mp4">
                                    </video>
                                </td>
                                <td width="75%" valign="top">
                                    <papertitle>Diffusion-based Affordance Prediction for Multi-modality Storage
                                    </papertitle>
                                    <p>
                                        Haonan Chang, Kowndinya Boyalakuntla, Yuhan Liu, <author>Xinyu Zhang</author>,
                                        Liam Schramm, Abdeslam Boularias
                                        <br>
                                        <i>International Conference on Intelligent Robots and Systems (IROS) 2024
                                        </i>
                                        <br>
                                        <a href="https://arxiv.org/abs/2409.00499" target="_blank">PDF</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://github.com/changhaonan/DPS" target="_blank">Code</a>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>



                    <table
                        style="width:100%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding-left:20px;padding-bottom:20px;width:33%;vertical-align:top">
                                    <img src="publications/iros2023/website.jpg" width="280" class="shadow-border"
                                        style="border-radius: 5px;">
                                </td>
                                <td width="75%" valign="top">
                                    <papertitle>Optical Flow boosts Unsupervised Localization and Segmentation
                                    </papertitle>
                                    <p>
                                        <author>Xinyu Zhang</author>, Abdeslam Boularias
                                        <br>
                                        <i>International Conference on Intelligent Robots and Systems (IROS) 2023
                                        </i>
                                        <br>
                                        <a href="https://arxiv.org/abs/2307.13640" target="_blank">PDF</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://github.com/mlzxy/flowdino" target="_blank">Code</a>
                                    </p>

                                    <div class="ppt">
                                        <ul>
                                            <li>Make DINO features more object-aware</li>
                                            <li>By using optical flow as regularization, i.e., similar local flow yields
                                                similar local features </li>
                                        </ul>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <table
                        style="width:100%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding-left:20px;padding-bottom:20px;width:33%;vertical-align:top">
                                    <img src="publications/mdpi2022/website.jpg" width="280" class="shadow-border"
                                        style="border-radius: 5px;">
                                </td>
                                <td width="75%" valign="top">
                                    <papertitle>Learning Low-Precision Structured Subnetworks Using Joint Layerwise
                                        Channel Pruning and Uniform Quantization
                                    </papertitle>
                                    <p>
                                        <author>Xinyu Zhang</author>, Ian Colbert, Srinjoy Das
                                        <br>
                                        <i>MDPI Journal Apply Science 2022
                                        </i>
                                        <br>
                                        <a href="https://www.mdpi.com/2076-3417/12/15/7829" target="_blank">PDF</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://github.com/mlzxy/qsparse" target="_blank">Code</a>
                                    </p>


                                    <div class="ppt">
                                        <ul>
                                            <li>Prune layers in topological orders, instead of all at once</li>
                                            <li>Because the neuron importance heavily depends on the sparsity of
                                                previous layers.</li>
                                        </ul>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>




                    <table
                        style="width:100%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding-left:20px;padding-bottom:20px;width:33%;vertical-align:top">
                                    <img src="publications/aaai2020/fig.jpg" width="280" class="shadow-border"
                                        style="border-radius: 5px;">
                                </td>
                                <td width="75%" valign="top">
                                    <papertitle>Diversity transfer network for few-shot learning
                                    </papertitle>
                                    <p>
                                        Mengting Chen, Yuxin Fang, Xinggang Wang, Heng Luo, Yifeng Geng, <author>Xinyu
                                            Zhang</author>, Chang Huang, Wenyu Liu, Bo Wang
                                        <br>
                                        <i>AAAI Conference on Artificial Intelligence 2020
                                        </i>
                                        <br>
                                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6628"
                                            target="_blank">PDF</a>
                                        &nbsp;•&nbsp;
                                        <a href="https://github.com/Yuxin-CV/DTN" target="_blank">Code</a>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>



                    <!-- Footer -->
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding:0px">
                        <tbody>
                            <tr>
                                <td>
                                    <hr>

                                    <p style="text-align:left;font-size:10px">
                                        <span style="font-size:10px;float:right">
                                            Layout from <a href="https://baichuan05.github.io/"
                                                style="font-size: 10px;">Baichuan Huang</a>.
                                        </span>
                                        Last update on Oct 2024
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </td>
            </tr>
        </tbody>
    </table>
</body>
